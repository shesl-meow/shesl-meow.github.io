<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分布式系统 on shesl-meow's note site</title><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</link><description>Recent content in 分布式系统 on shesl-meow's note site</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/1.introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/1.introduction/</guid><description>Introduction # 分布式系统的优点：
Parallelism：通过并行提高计算量、数据处理量； Fault Tolerate：通过多个服务器提升容错率； Physically：聚合物理意义上相互隔离的机器； Security：系统可以分布式地运行在地理位置相聚很远的地方，提升容灾性； 分布式系统的缺点与困难：
Concurrent Parts：需要同时并行地处理多个部分； Complex Interactions：需要处理复杂的服务交互逻辑（微服务中通过“服务发现”实现分布式系统）； Partial Failure：无法同时达到绝对准确性与服务可用性； Performance：需要设计精巧的算法以实现更高的性能； MapReduce 是“分布式系统”的开山论文，它最初被 Google 研发出来，用于解决大容量的网页问题：http://nil.csail.mit.edu/6.824/2020/papers/mapreduce.pdf</description></item><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2.infra-rpc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2.infra-rpc/</guid><description>Infrastrcture: RPC and threads # 为什么要选择 Go 语言？
通过 go 与 goroutine 对并发有很好的支持； 简易的 RPC 框架； 类型安全（相对于 js、python 这类的脚本语言）； 自动垃圾回收（没有 UAF 漏洞）； 相对简单的语法（工业界需要一个像 python 一样简单语法的编译型语言）； 多线程的挑战：
Share Data：Golang 中使用 sync.Mutex，尽量避免共享可变量； Coordination：Glang 使用 channel、sync.Cond、WaitGroup； Deallock：线程间的循环等待； lock/channel 应该在什么时候？
一个结论是：大部分的多线程问题可以通过上面两个工具解决； 什么时候使用哪个工具取决于程序员的思考，一个建议是： state 状态共享时：使用 lock； communication 线程间信息交流时：使用 channel； RPC：Remote Procedure Call；
Client &amp;ldquo;best effort&amp;rdquo;：当服务端无响应时，多进行几次尝试后抛出异常； Server &amp;ldquo;at most once&amp;rdquo;：客户端对每次请求添加 xid 唯一表示，服务端通过 xid 去重；</description></item><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/3.gfs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/3.gfs/</guid><description>Google File System (Distribute Storage) # 分布式存储的困难点：
高性能：High performance in many server; 多机器：System with many machine could cause &amp;ldquo;constant fault&amp;rdquo;; 一致性错误：To avoid contact fault, we will need replication; 数据同步：During the replication, potential inconsistencies will occur; 一致性与高性能矛盾：To get better consistency, low performance occur; 分布式存储的一个大的课题就是在“一致性”与“高性能”之间的 tradeoff.
GFS Master：
RAM 中存储： 一个 filename 到 handlers array 的映射表； 每个 handler 都包含 version/chunk servers list/primary/least time 信息； 磁盘中存储着 log/checkpoint； READ，客户端读取流程：</description></item><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/4.vm-ft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/4.vm-ft/</guid><description>VMware FT(Fault-Tolerant) # Primary/Backup Replication # Two main replication approaches:
State Transfer: Primary replica executes and sends new state to backups machine; Replicated State Machine: Primary just pass the raw external event to backups. Mostly used by recent industry and papers; Overview:
VM-FT consist of two machine: primary and backup. Primary deals with all external events and replicates it to backup through &amp;ldquo;logging channel&amp;rdquo;; VM-FT emulates a local disk interface through two remote disk server.</description></item><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/5.go-raft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/5.go-raft/</guid><description>Go Memory Model: https://golang.org/ref/mem
Go Threads and Raft # Happens Before # Hanpens Before，为了深入研究同步问题而提出的概念：
关于一个变量 v 的读语句 r 与写语句 w，r 可以得到 w 的值需要满足以下条件： r does not happene before w; There is not other w' that happens before r and after w; 一个读语句 v 能够准确地捕获到某个特定的 w 语句的值，需要满足以下条件： w happends before r; Any other w' to v either hanppends before w or after r; Golang Lifetime # Initialization, 一个 golang 程序的初始化：</description></item><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/6.raft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/6.raft/</guid><description>论文地址：https://raft.github.io/raft.pdf
Raft # Split Brain # 之前学习过的系统中：
MapReduce：replicates computation but relies on a single master to organize; GFS：replicates data but relies on the master to pick primaries; VMwareFT：replicates service but relies on test-and-set to pick primary; 上面这些系统在做核心决策的时候都需要依赖单一的机器，也就是通过单一的机器避免 Split Brain 问题。
为什么会出现 Split Brain 问题呢？
根本原因在于计算机之间无法区分 server crashed 与 network broken 这两个情况。 比如：一个系统中的一个机器 A 无法与另一个机器 B 通信了，如果 A 认为 B 是宕机了而实际上是网络错误，反之亦然。A/B 就分裂成了两个独立的服务，它们都认为对方宕机而自己继续服务客户端请求； &amp;ldquo;Split Brain&amp;rdquo; caused by &amp;ldquo;network partition&amp;rdquo; seemed insurmountable for a long time:</description></item><item><title/><link>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/7.zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shesl-meow.github.io/docs/%E7%90%86%E8%AE%BA%E8%AF%BE%E7%A8%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/7.zookeeper/</guid><description>Paper: https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf
ZooKeeper # What questions does this paper shed light on?
我们是否能将 Raft 中提到的服务间合作封装成一个通用的服务？
如果可以，API 应该设计成什么样？其他的分布式系统应该怎么使用这个服务？
我们在一个分布式系统总投入了 N 倍的机器，能够得到 N 倍的性能提升？
Performance # Raft：在添加了更多的 replicas 之后，因为 Leader 需要等待响应的机器增多，反而会降低性能。ZooKeeper 提高性能的一个基本思想是：
将 Read 负载分散到各个 Replicas 机器中，使得读性能能够随机器数量线性地提升； 但是在传统的 Raft 架构下的，直接读取 Replicas 会遇到一些问题： Replica may not be in majority, so may not have seen a completed write; Replica may not yet have seen a commit for a completed write; Replica may be entirely cut off from the leader; How does ZooKeeper skin this cat?</description></item></channel></rss>